{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Create batch pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Connect to your workspace\r\n",
    "from azureml.core import Workspace\r\n",
    "ws = Workspace.from_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Provision inference compute"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "\r\n",
    "cluster_name = \"mlopsbootcamp\"\r\n",
    "\r\n",
    "try:\r\n",
    "    # Check for existing compute target\r\n",
    "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
    "    print('Found existing cluster, use it.')\r\n",
    "except ComputeTargetException:\r\n",
    "    # If it doesn't already exist, create it\r\n",
    "    try:\r\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS2_v2', max_nodes=2)\r\n",
    "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "        inference_cluster.wait_for_completion(show_output=True)\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating......\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a pipeline for batch inferencing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\r\n",
    "# Create a folder for the experiment files\r\n",
    "experiment_folder = 'batch_pipeline'\r\n",
    "os.makedirs(experiment_folder, exist_ok=True)\r\n",
    "\r\n",
    "print(experiment_folder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch_pipeline\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get path to the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from azureml.core import Model\r\n",
    "model_list = Model.list(ws)\r\n",
    "model_path = Model.get_model_path('fourier_regression')\r\n",
    "print(model_list, model_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Model(workspace=Workspace.create(name='mlworkspace', subscription_id='e7d71274-b7c4-47ed-9751-2505b563b918', resource_group='mlgroup'), name=fourier_regression, id=fourier_regression:1, version=1, tags={}, properties={})] azureml-models\\fourier_regression\\2\\fourier.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import joblib\r\n",
    "model = joblib.load(model_path)\r\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the batch data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "mini_batch = list()\r\n",
    "for (dirpath, dirnames, filenames) in os.walk(\"batch-data\"):\r\n",
    "    mini_batch += [os.path.join(dirpath, file) for file in filenames]\r\n",
    "for elem in mini_batch:\r\n",
    "    print(elem)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch-data\\1.csv\n",
      "batch-data\\2.csv\n",
      "batch-data\\3.csv\n",
      "batch-data\\4.csv\n",
      "batch-data\\5.csv\n",
      "batch-data\\6.csv\n",
      "batch-data\\7.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make prediction per batch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\r\n",
    "def run(mini_batch):\r\n",
    "    # This runs for each batch\r\n",
    "    resultList = []\r\n",
    "\r\n",
    "    # process each file in the batch\r\n",
    "    for f in mini_batch:\r\n",
    "        # Read comma-delimited data into an array\r\n",
    "        data = np.genfromtxt(f, delimiter=',')\r\n",
    "        print(data)\r\n",
    "        # Reshape into a 2-dimensional array for model input\r\n",
    "        prediction = model.predict(data.reshape(1, -1))\r\n",
    "        # Append prediction to results\r\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\r\n",
    "    return resultList\r\n",
    "\r\n",
    "result = run(mini_batch)\r\n",
    "result"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.88624084e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[2.88446014e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[2.88254852e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[2.88050537e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[2.87846252e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[2.87641937e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[2.87930267e+02 0.00000000e+00 2.58819045e-01 9.65925826e-01\n",
      " 5.00000000e-01 8.66025404e-01 7.07106781e-01 7.07106781e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1.csv: 95.15347618619145',\n",
       " '2.csv: 95.2173844857991',\n",
       " '3.csv: 95.28599144188169',\n",
       " '4.csv: 95.35931895959774',\n",
       " '5.csv: 95.43263552473465',\n",
       " '6.csv: 95.5059630424507',\n",
       " '7.csv: 95.40248307420845']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarize into python script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%writefile $experiment_folder\\score.py\r\n",
    "\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "from azureml.core import Model\r\n",
    "import joblib\r\n",
    "\r\n",
    "def init():\r\n",
    "    # Runs when the pipeline step is initialized\r\n",
    "    global model\r\n",
    "\r\n",
    "    # load the model\r\n",
    "    model_path = Model.get_model_path('fourier_regression')\r\n",
    "    model = joblib.load(model_path)\r\n",
    "\r\n",
    "def run(mini_batch):\r\n",
    "    # This runs for each batch\r\n",
    "    resultList = []\r\n",
    "\r\n",
    "    # process each file in the batch\r\n",
    "    for f in mini_batch:\r\n",
    "        # Read comma-delimited data into an array\r\n",
    "        data = np.genfromtxt(f, delimiter=',')\r\n",
    "        # Reshape into a 2-dimensional array for model input\r\n",
    "        prediction = model.predict(data.reshape(1, -1))\r\n",
    "        # Append prediction to results\r\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\r\n",
    "    return resultList"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting batch_pipeline\\score.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create conda environment for the pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%%writefile $experiment_folder\\batch_environment.yml\r\n",
    "name: batch_environment\r\n",
    "dependencies:\r\n",
    "- python=3.8\r\n",
    "- numpy\r\n",
    "- pandas\r\n",
    "- scikit-learn\r\n",
    "- pip:\r\n",
    "    - azureml-core\r\n",
    "    - azureml-dataset-runtime[fuse]\r\n",
    "    - azureml-pipeline-core\r\n",
    "    - azureml-pipeline-steps"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting batch_pipeline\\batch_environment.yml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define run using environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from azureml.core import Environment\r\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
    "\r\n",
    "# Create an Environment for the experiment\r\n",
    "batch_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/batch_environment.yml\")\r\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
    "print('Configuration ready.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuration ready.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure batch pipeline steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from datetime import datetime\r\n",
    "\r\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "\r\n",
    "# Get the batch dataset for input\r\n",
    "batch_data_set = ws.datasets['batch-data']\r\n",
    "\r\n",
    "# Set the output location\r\n",
    "default_ds = ws.get_default_datastore()\r\n",
    "output_dir = OutputFileDatasetConfig(name='inferences')\r\n",
    "\r\n",
    "# Define the parallel run step step configuration\r\n",
    "parallel_run_config = ParallelRunConfig(\r\n",
    "    source_directory=experiment_folder,\r\n",
    "    entry_script=\"score.py\",\r\n",
    "    mini_batch_size=\"5\",\r\n",
    "    error_threshold=10,\r\n",
    "    output_action=\"append_row\",\r\n",
    "    environment=batch_env,\r\n",
    "    compute_target=inference_cluster,\r\n",
    "    node_count=2)\r\n",
    "\r\n",
    "parallel_step_name = \"batchscoring-\" + datetime.now().strftime(\"%Y%m%d%H%M\")\r\n",
    "\r\n",
    "# Create the parallel run step\r\n",
    "parallelrun_step = ParallelRunStep(\r\n",
    "    name=parallel_step_name,\r\n",
    "    parallel_run_config=parallel_run_config,\r\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\r\n",
    "    output=output_dir,\r\n",
    "    arguments=[],\r\n",
    "    allow_reuse=True\r\n",
    ")\r\n",
    "\r\n",
    "print('Steps defined')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Steps defined\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from azureml.core import Experiment\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "\r\n",
    "# Create the pipeline\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\r\n",
    "\r\n",
    "# Run the pipeline as an experiment\r\n",
    "pipeline_run = Experiment(ws, 'pytown-energy-demand-batch').submit(pipeline)\r\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step batchscoring-202108011650 [4b275680][e9771bb7-d04b-4e22-815f-f957ee852bc1], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 89a52746-e0f9-4f08-8448-dca1ad15f76a\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/89a52746-e0f9-4f08-8448-dca1ad15f76a?wsid=/subscriptions/e7d71274-b7c4-47ed-9751-2505b563b918/resourcegroups/mlgroup/workspaces/mlworkspace&tid=a0f1cacd-618c-4403-b945-76fb3d6874e5\n",
      "PipelineRunId: 89a52746-e0f9-4f08-8448-dca1ad15f76a\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/89a52746-e0f9-4f08-8448-dca1ad15f76a?wsid=/subscriptions/e7d71274-b7c4-47ed-9751-2505b563b918/resourcegroups/mlgroup/workspaces/mlworkspace&tid=a0f1cacd-618c-4403-b945-76fb3d6874e5\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 32b13e31-6945-4306-ac84-a964baab43c4\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/32b13e31-6945-4306-ac84-a964baab43c4?wsid=/subscriptions/e7d71274-b7c4-47ed-9751-2505b563b918/resourcegroups/mlgroup/workspaces/mlworkspace&tid=a0f1cacd-618c-4403-b945-76fb3d6874e5\n",
      "StepRun( batchscoring-202108011650 ) Status: NotStarted\n",
      "StepRun( batchscoring-202108011650 ) Status: Queued\n",
      "\n",
      "StepRun(batchscoring-202108011650) Execution Summary\n",
      "=====================================================\n",
      "StepRun( batchscoring-202108011650 ) Status: Failed\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Response status code does not indicate success: 400 (You don't have permissions to do this operation on storage. Make sure you didn't change the permissi).\\nMicrosoft.RelInfra.Common.Exceptions.ErrorResponseException: You don't have permissions to do this operation on storage. Make sure you didn't change the permissions to storage\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Response status code does not indicate success: 400 (You don't have permissions to do this operation on storage. Make sure you didn't change the permissi).\\\\nMicrosoft.RelInfra.Common.Exceptions.ErrorResponseException: You don't have permissions to do this operation on storage. Make sure you didn't change the permissions to storage\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-af6b5d103b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Run the pipeline as an experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpipeline_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pytown-energy-demand-batch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpipeline_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlops\\lib\\site-packages\\azureml\\pipeline\\core\\run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[1;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[0;32m    292\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0m\u001b[0;32m    295\u001b[0m                                                              raise_on_error=raise_on_error)\n\u001b[0;32m    296\u001b[0m                             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlops\\lib\\site-packages\\azureml\\pipeline\\core\\run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[1;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0m\u001b[0;32m    737\u001b[0m                                                raise_on_error=raise_on_error)\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlops\\lib\\site-packages\\azureml\\pipeline\\core\\run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[1;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Response status code does not indicate success: 400 (You don't have permissions to do this operation on storage. Make sure you didn't change the permissi).\\nMicrosoft.RelInfra.Common.Exceptions.ErrorResponseException: You don't have permissions to do this operation on storage. Make sure you didn't change the permissions to storage\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Response status code does not indicate success: 400 (You don't have permissions to do this operation on storage. Make sure you didn't change the permissi).\\\\nMicrosoft.RelInfra.Common.Exceptions.ErrorResponseException: You don't have permissions to do this operation on storage. Make sure you didn't change the permissions to storage\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import shutil\r\n",
    "\r\n",
    "# Remove the local results folder if left over from a previous run\r\n",
    "shutil.rmtree('batch-results', ignore_errors=True)\r\n",
    "\r\n",
    "# Get the run for the first step and download its output\r\n",
    "prediction_run = next(pipeline_run.get_children())\r\n",
    "prediction_output = prediction_run.get_output_data('inferences')\r\n",
    "prediction_output.download(local_path='batch-results')\r\n",
    "\r\n",
    "# Traverse the folder hierarchy and find the results file\r\n",
    "for root, dirs, files in os.walk('batch-results'):\r\n",
    "    for file in files:\r\n",
    "        if file.endswith('parallel_run_step.txt'):\r\n",
    "            result_file = os.path.join(root,file)\r\n",
    "\r\n",
    "# cleanup output format\r\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\r\n",
    "df.columns = [\"File\", \"Prediction\"]\r\n",
    "\r\n",
    "# Display the first 20 results\r\n",
    "df.head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Publish the pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# published_pipeline = pipeline_run.publish_pipeline(name='Fourier_regression_batch_prediction_pipeline',\r\n",
    "#                                                    description='Batch scoring using linear regression model with Fourier ML features',\r\n",
    "#                                                    version='1.0')\r\n",
    "\r\n",
    "# published_pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get REST endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# rest_endpoint = published_pipeline.endpoint\r\n",
    "# print(rest_endpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schedule the pipeline to run every Monday at 04:00 in the morning (02:00 UTC)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from azureml.pipeline.core import ScheduleRecurrence, Schedule\r\n",
    "\r\n",
    "# weekly = ScheduleRecurrence(frequency='Week', interval=1, week_days=[\"Monday\"], time_of_day=\"02:00\")\r\n",
    "# pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\r\n",
    "#                                         description='batch inferencing',\r\n",
    "#                                         pipeline_id=published_pipeline.id,\r\n",
    "#                                         experiment_name='Batch_Prediction',\r\n",
    "#                                         recurrence=weekly)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Disable pipeline with active schedule"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ss = Schedule.list(ws)\r\n",
    "# for s in ss:\r\n",
    "#     print(s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def stop_by_schedule_id(ws, schedule_id):\r\n",
    "#     s = next(s for s in Schedule.list(ws) if s.id == schedule_id)\r\n",
    "#     s.disable()\r\n",
    "#     return s\r\n",
    "\r\n",
    "# schedule_id = 'e14c5ce5-db2c-462b-97f4-8c1c21faa# 31b'\r\n",
    "# stop_by_schedule_id(ws, schedule_id)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0c0641573e26ff90e279ea3d930a41116c682b71b3d26a209012b19edb32094"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mlops': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}